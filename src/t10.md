Sure, let's add web scraping and compilation tools in a similar way to your `ProbeTool` example. We'll create `WebScrapingTool` and `CompilationTool` classes that can be used with Langroid agents.

### Web Scraping Tool

```python
class WebScrapingTool(lr.agent.ToolMessage):
    request: str = "scrape" 
    purpose: str = """ 
        To scrape data from the specified URL and return the content
        """ 
    url: str 

    @classmethod
    def examples(cls): 
        return [
            cls(url="https://example-blog.com/post1"),
            (
                "Scrape content from the specified paper",
                cls(url="https://example-paper.com/paper1.pdf"),
            )
        ]

    def scrape_blogs(self, url):
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')
        return soup.get_text()

    def scrape_papers(self, url):
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')
        return soup.get_text()

    def scrape_pdfs(self, file_path):
        with open(file_path, 'rb') as file:
            reader = PyPDF2.PdfFileReader(file)
            text = ''
            for page_num in range(reader.getNumPages()):
                text += reader.getPage(page_num).extract_text()
        return text
```

### Compilation Tool

```python
class CompilationTool(lr.agent.ToolMessage):
    request: str = "compile" 
    purpose: str = """ 
        To compile the provided chapters into a book and convert to PDF
        """ 
    chapters: list 
    title: str 
    cover_image: str 
    toc: bool 

    @classmethod
    def examples(cls): 
        return [
            cls(
                chapters=["Chapter 1 text", "Chapter 2 text"],
                title="Trends in LLMs",
                cover_image="cover.png",
                toc=True
            ),
            (
                "Compile the chapters into a book with table of contents",
                cls(
                    chapters=["Chapter A", "Chapter B"],
                    title="LLM Insights",
                    cover_image="llm_cover.png",
                    toc=False
                ),
            )
        ]

    def compile_book(self, chapters, title, cover_image, toc):
        book_content = f"# {title}\n\n"
        for i, chapter in enumerate(chapters):
            book_content += f"## Chapter {i+1}\n\n{chapter}\n\n"
        
        with open('book.md', 'w') as file:
            file.write(book_content)
        
        # Compile the book using Pandoc to convert the markdown file into a PDF
        subprocess.run(['pandoc', 'book.md', '-o', 'book.pdf', '--toc' if toc else '', '--metadata=title={title}', '--metadata=author=Author Name'])
```

### Integrating the Tools into Langroid Agents

#### Web Scraping Agent

```python
web_scraping_config = lr.ChatAgentConfig(
    name="WebScrapingAgent",
    llm=lr.language_models.OllamaConfig(
        chat_model=lr.language_models.OllamaModel.LLaMa3,
    ),
    system_message="You are a web scraping assistant",
    tools=[WebScrapingTool]
)
web_scraping_agent = lr.ChatAgent(web_scraping_config)
```

#### Compilation Agent

```python
compilation_config = lr.ChatAgentConfig(
    name="CompilationAgent",
    llm=lr.language_models.OllamaConfig(
        chat_model=lr.language_models.OllamaModel.LLaMa3,
    ),
    system_message="You are a book compilation assistant",
    tools=[CompilationTool]
)
compilation_agent = lr.ChatAgent(compilation_config)
```

### Controller to Manage Agents

```python
class BookWritingController:
    def __init__(self):
        self.web_scraping_agent = web_scraping_agent
        self.data_organization_agent = data_organization_agent
        self.summarization_agent = summarization_agent
        self.writer_agent = writer_agent
        self.reviewer_agent = reviewer_agent
        self.revision_agent = revision_agent
        self.compilation_agent = compilation_agent

    def write_book(self, sources):
        raw_data = [self.web_scraping_agent.scrape_blogs(source) for source in sources]
        organized_data = self.data_organization_agent.organize_data(raw_data)
        summaries = [self.summarization_agent.summarize_content(data['Content']) for _, data in organized_data.iterrows()]
        chapters = [self.writer_agent.write_chapter(summary) for summary in summaries]
        reviewed_chapters = [self.reviewer_agent.review_chapter(chapter) for chapter in chapters]
        revised_chapters = [self.revision_agent.rewrite_chapter(chapter, review) for chapter, review in zip(chapters, reviewed_chapters)]
        self.compilation_agent.compile_book(revised_chapters, title='Trends in LLMs', cover_image='cover.png', toc=True)

# Usage
controller = BookWritingController()
controller.write_book(['https://example-blog.com/post1', 'https://example-paper.com/paper1.pdf'])
```

In this setup:
- The **Web Scraping Tool** within the `WebScrapingAgent` class handles the web scraping tasks.
- The **Compilation Tool** within the `CompilationAgent` class handles compiling the chapters into a book using Pandoc.

Each agent is equipped with tools that define specific requests, purposes, and examples to guide their operations. The `BookWritingController` manages the workflow from data collection to book compilation.